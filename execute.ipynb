{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modi1.wav\n",
      "modi1.wav\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist, euclidean, cosine\n",
    "from glob import glob\n",
    "\n",
    "from model import vggvox_model\n",
    "from wav_reader import get_fft_spectrum\n",
    "import constants as c\n",
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "def build_buckets(max_sec, step_sec, frame_step):\n",
    "    buckets = {}\n",
    "    frames_per_sec = int(1/frame_step)\n",
    "    end_frame = int(max_sec*frames_per_sec)\n",
    "    step_frame = int(step_sec*frames_per_sec)\n",
    "    for i in range(0, end_frame+1, step_frame):\n",
    "        s = i\n",
    "        s = np.floor((s-7+2)/2) + 1  # conv1\n",
    "        s = np.floor((s-3)/2) + 1  # mpool1\n",
    "        s = np.floor((s-5+2)/2) + 1  # conv2\n",
    "        s = np.floor((s-3)/2) + 1  # mpool2\n",
    "        s = np.floor((s-3+2)/1) + 1  # conv3\n",
    "        s = np.floor((s-3+2)/1) + 1  # conv4\n",
    "        s = np.floor((s-3+2)/1) + 1  # conv5\n",
    "        s = np.floor((s-3)/2) + 1  # mpool5\n",
    "        s = np.floor((s-1)/1) + 1  # fc6\n",
    "        if s > 0:\n",
    "            buckets[i] = int(s)\n",
    "    return buckets\n",
    "\n",
    "\n",
    "def get_embeddings_from_list_file(model, list_file, max_sec):\n",
    "    buckets = build_buckets(max_sec, c.BUCKET_STEP, c.FRAME_STEP)\n",
    "    result = pd.read_csv(list_file,delimiter=\",\",names=['filename','speaker'])\n",
    "    result['features'] = result['filename'].apply(lambda x: get_fft_spectrum(x, buckets))\n",
    "    result['embedding'] = result['features'].apply(lambda x: np.squeeze(model.predict(x.reshape(1,*x.shape,1))))   \n",
    "    return result[['filename','speaker','embedding']]\n",
    "\n",
    "def get_id_result():\n",
    "    #print(\"Loading model weights from [{}]....\".format(c.WEIGHTS_FILE))\n",
    "    model = vggvox_model()\n",
    "    model.load_weights(c.WEIGHTS_FILE)\n",
    "    #model.summary()\n",
    "    #print(\"Processing enroll samples....\")\n",
    "    enroll_result = get_embeddings_from_list_file(model, c.ENROLL_LIST_FILE, c.MAX_SEC)\n",
    "    enroll_embs = np.array([emb.tolist() for emb in enroll_result['embedding']])\n",
    "    speakers = enroll_result['speaker']\n",
    "    #print(\"Processing test samples....\")\n",
    "    test_result = get_embeddings_from_list_file(model, c.TEST_LIST_FILE, c.MAX_SEC)\n",
    "    test_embs = np.array([emb.tolist() for emb in test_result['embedding']])\n",
    "    #print(\"Comparing test samples against enroll samples....\")\n",
    "    distances = pd.DataFrame(cdist(test_embs, enroll_embs, metric=c.COST_METRIC),columns=speakers)\n",
    "    dist_t=pd.DataFrame.transpose(distances)\n",
    "    dist_t=np.array(dist_t)\n",
    "    k=np.amin(dist_t,axis=0)\n",
    "    if(k[0]>.22):\n",
    "        #by using cosine distance if value if less than 0.2 then it proceed further otherwise halting the proghram.\n",
    "        print(\"unauthorized user................\")\n",
    "    else:\n",
    "        en=enroll_result.loc[:,'filename']\n",
    "        d1=pd.DataFrame(dist_t,columns=['distance'])\n",
    "        scores = pd.concat([en, d1],axis=1)\n",
    "        #taking only most similar audio file\n",
    "        aud=scores[scores['distance']==k[0]]\n",
    "        k1=list(aud['filename'])\n",
    "        re= pd.read_csv(c.TEST_LIST_FILE,names=['filename'])\n",
    "        #taking the single test file.\n",
    "        k2=list(re['filename'])\n",
    "        print(k1[0])\n",
    "        AUDIO_FILE_1 = k1[0]\n",
    "        AUDIO_FILE_2=k2[0]\n",
    "        print(k2[0])\n",
    "        #now we are going to match the text using google speech to text api.\n",
    "        # use the audio file as the audio source                                        \n",
    "        r = sr.Recognizer()\n",
    "        a=[]\n",
    "        with sr.AudioFile(AUDIO_FILE_1) as source:\n",
    "            audio = r.record(source) \n",
    "            a=r.recognize_google(audio)\n",
    "            a=list(a.split(\" \"))\n",
    "            l1=len(a)\n",
    "        b=[]\n",
    "        with sr.AudioFile(AUDIO_FILE_2) as source:\n",
    "            audio = r.record(source) \n",
    "            b=r.recognize_google(audio)\n",
    "            b=list(b.split(\" \"))\n",
    "            l2=len(b)\n",
    "        counter=0\n",
    "        for i in range(l1):\n",
    "            if(a[i]==b[i]):\n",
    "                counter+=1\n",
    "        if(l1>=l2):\n",
    "            denom=l2\n",
    "        else:\n",
    "            denom=l1\n",
    "        if(counter/denom>0.8):\n",
    "            #if usre is authorized by both speaker validation and text matching\n",
    "            print(\"true\")\n",
    "        else:\n",
    "            # if text does not match.\n",
    "            print(\"false\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_id_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Aman Jaiswal\\\\Desktop\\\\final\\\\vgg'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
